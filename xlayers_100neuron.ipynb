{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad3ca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, begin evaluation.\n",
      "Epoch: 10, begin evaluation.\n",
      "Epoch: 20, begin evaluation.\n",
      "Epoch: 30, begin evaluation.\n",
      "Epoch: 40, begin evaluation.\n",
      "Epoch: 50, begin evaluation.\n",
      "Epoch: 60, begin evaluation.\n",
      "Epoch: 70, begin evaluation.\n",
      "Epoch: 80, begin evaluation.\n",
      "Epoch: 90, begin evaluation.\n",
      "Epoch: 100, begin evaluation.\n",
      "Epoch: 110, begin evaluation.\n",
      "Epoch: 120, begin evaluation.\n",
      "Epoch: 130, begin evaluation.\n",
      "Epoch: 140, begin evaluation.\n",
      "Epoch: 150, begin evaluation.\n",
      "Epoch: 160, begin evaluation.\n",
      "Epoch: 170, begin evaluation.\n",
      "Epoch: 180, begin evaluation.\n",
      "Epoch: 190, begin evaluation.\n",
      "Epoch: 200, begin evaluation.\n",
      "Epoch: 210, begin evaluation.\n",
      "Epoch: 220, begin evaluation.\n",
      "Epoch: 230, begin evaluation.\n",
      "Epoch: 240, begin evaluation.\n",
      "Epoch: 250, begin evaluation.\n",
      "Epoch: 260, begin evaluation.\n",
      "Epoch: 270, begin evaluation.\n",
      "Epoch: 280, begin evaluation.\n",
      "Epoch: 290, begin evaluation.\n",
      "Epoch: 300, begin evaluation.\n",
      "Epoch: 310, begin evaluation.\n",
      "Epoch: 320, begin evaluation.\n",
      "Epoch: 330, begin evaluation.\n",
      "Epoch: 340, begin evaluation.\n",
      "Epoch: 350, begin evaluation.\n",
      "Epoch: 360, begin evaluation.\n",
      "Epoch: 370, begin evaluation.\n",
      "Epoch: 380, begin evaluation.\n",
      "Epoch: 390, begin evaluation.\n",
      "Epoch: 400, begin evaluation.\n",
      "Epoch: 410, begin evaluation.\n",
      "Epoch: 420, begin evaluation.\n",
      "Epoch: 430, begin evaluation.\n",
      "Epoch: 440, begin evaluation.\n",
      "Epoch: 450, begin evaluation.\n",
      "Epoch: 460, begin evaluation.\n",
      "Epoch: 470, begin evaluation.\n",
      "Epoch: 480, begin evaluation.\n",
      "Epoch: 490, begin evaluation.\n",
      "Epoch: 500, begin evaluation.\n",
      "Epoch: 510, begin evaluation.\n",
      "Epoch: 520, begin evaluation.\n",
      "Epoch: 530, begin evaluation.\n",
      "Epoch: 540, begin evaluation.\n",
      "Epoch: 550, begin evaluation.\n",
      "Epoch: 560, begin evaluation.\n",
      "Epoch: 570, begin evaluation.\n",
      "Epoch: 580, begin evaluation.\n",
      "Epoch: 590, begin evaluation.\n",
      "Epoch: 600, begin evaluation.\n",
      "Epoch: 610, begin evaluation.\n",
      "Epoch: 620, begin evaluation.\n",
      "Epoch: 630, begin evaluation.\n",
      "Epoch: 640, begin evaluation.\n",
      "Epoch: 650, begin evaluation.\n",
      "Epoch: 660, begin evaluation.\n",
      "Epoch: 670, begin evaluation.\n",
      "Epoch: 680, begin evaluation.\n",
      "Epoch: 690, begin evaluation.\n",
      "Epoch: 700, begin evaluation.\n",
      "Epoch: 710, begin evaluation.\n",
      "Epoch: 720, begin evaluation.\n",
      "Epoch: 730, begin evaluation.\n",
      "Epoch: 740, begin evaluation.\n",
      "Epoch: 750, begin evaluation.\n",
      "Epoch: 760, begin evaluation.\n",
      "Epoch: 770, begin evaluation.\n",
      "Epoch: 780, begin evaluation.\n",
      "Epoch: 790, begin evaluation.\n",
      "Epoch: 800, begin evaluation.\n",
      "Epoch: 810, begin evaluation.\n",
      "Epoch: 820, begin evaluation.\n",
      "Epoch: 830, begin evaluation.\n",
      "Epoch: 840, begin evaluation.\n",
      "Epoch: 850, begin evaluation.\n",
      "Epoch: 860, begin evaluation.\n",
      "Epoch: 870, begin evaluation.\n",
      "Epoch: 880, begin evaluation.\n",
      "Epoch: 890, begin evaluation.\n",
      "Epoch: 900, begin evaluation.\n",
      "Epoch: 910, begin evaluation.\n",
      "Epoch: 920, begin evaluation.\n",
      "Epoch: 930, begin evaluation.\n",
      "Epoch: 940, begin evaluation.\n",
      "Epoch: 950, begin evaluation.\n",
      "Epoch: 960, begin evaluation.\n",
      "Epoch: 970, begin evaluation.\n",
      "Epoch: 980, begin evaluation.\n",
      "Epoch: 990, begin evaluation.\n",
      "Epoch: 1000, begin evaluation.\n",
      "Epoch: 1010, begin evaluation.\n",
      "Epoch: 1020, begin evaluation.\n",
      "Epoch: 1030, begin evaluation.\n",
      "Epoch: 1040, begin evaluation.\n",
      "Epoch: 1050, begin evaluation.\n",
      "Epoch: 1060, begin evaluation.\n",
      "Epoch: 1070, begin evaluation.\n",
      "Epoch: 1080, begin evaluation.\n",
      "Epoch: 1090, begin evaluation.\n",
      "Epoch: 1100, begin evaluation.\n",
      "Epoch: 1110, begin evaluation.\n",
      "Epoch: 1120, begin evaluation.\n",
      "Epoch: 1130, begin evaluation.\n",
      "Epoch: 1140, begin evaluation.\n",
      "Epoch: 1150, begin evaluation.\n",
      "Epoch: 1160, begin evaluation.\n",
      "Epoch: 1170, begin evaluation.\n",
      "Epoch: 1180, begin evaluation.\n",
      "Epoch: 1190, begin evaluation.\n",
      "Epoch: 1200, begin evaluation.\n",
      "Epoch: 1210, begin evaluation.\n",
      "Epoch: 1220, begin evaluation.\n",
      "Epoch: 1230, begin evaluation.\n",
      "Epoch: 1240, begin evaluation.\n",
      "Epoch: 1250, begin evaluation.\n",
      "Epoch: 1260, begin evaluation.\n",
      "Epoch: 1270, begin evaluation.\n",
      "Epoch: 1280, begin evaluation.\n",
      "Epoch: 1290, begin evaluation.\n",
      "Epoch: 1300, begin evaluation.\n",
      "Epoch: 1310, begin evaluation.\n",
      "Epoch: 1320, begin evaluation.\n",
      "Epoch: 1330, begin evaluation.\n",
      "Epoch: 1340, begin evaluation.\n",
      "Epoch: 1350, begin evaluation.\n",
      "Epoch: 1360, begin evaluation.\n",
      "Epoch: 1370, begin evaluation.\n",
      "Epoch: 1380, begin evaluation.\n",
      "Epoch: 1390, begin evaluation.\n",
      "Epoch: 1400, begin evaluation.\n",
      "Epoch: 1410, begin evaluation.\n",
      "Epoch: 1420, begin evaluation.\n",
      "Epoch: 1430, begin evaluation.\n",
      "Epoch: 1440, begin evaluation.\n",
      "Epoch: 1450, begin evaluation.\n",
      "Epoch: 1460, begin evaluation.\n",
      "Epoch: 1470, begin evaluation.\n",
      "Epoch: 1480, begin evaluation.\n",
      "Epoch: 1490, begin evaluation.\n",
      "Epoch: 1500, begin evaluation.\n",
      "Epoch: 1510, begin evaluation.\n",
      "Epoch: 1520, begin evaluation.\n",
      "Epoch: 1530, begin evaluation.\n",
      "Epoch: 1540, begin evaluation.\n",
      "Epoch: 1550, begin evaluation.\n",
      "Epoch: 1560, begin evaluation.\n",
      "Epoch: 1570, begin evaluation.\n",
      "Epoch: 1580, begin evaluation.\n",
      "Epoch: 1590, begin evaluation.\n",
      "Epoch: 1600, begin evaluation.\n",
      "Epoch: 1610, begin evaluation.\n",
      "Epoch: 1620, begin evaluation.\n",
      "Epoch: 1630, begin evaluation.\n",
      "Epoch: 1640, begin evaluation.\n",
      "Epoch: 1650, begin evaluation.\n",
      "Epoch: 1660, begin evaluation.\n",
      "Epoch: 1670, begin evaluation.\n",
      "Epoch: 1680, begin evaluation.\n",
      "Epoch: 1690, begin evaluation.\n",
      "Epoch: 1700, begin evaluation.\n",
      "Epoch: 1710, begin evaluation.\n",
      "Epoch: 1720, begin evaluation.\n",
      "Epoch: 1730, begin evaluation.\n",
      "Epoch: 1740, begin evaluation.\n",
      "Epoch: 1750, begin evaluation.\n",
      "Epoch: 1760, begin evaluation.\n",
      "Epoch: 1770, begin evaluation.\n",
      "Epoch: 1780, begin evaluation.\n",
      "Epoch: 1790, begin evaluation.\n",
      "Epoch: 1800, begin evaluation.\n",
      "Epoch: 1810, begin evaluation.\n",
      "Epoch: 1820, begin evaluation.\n",
      "Epoch: 1830, begin evaluation.\n",
      "Epoch: 1840, begin evaluation.\n",
      "Epoch: 1850, begin evaluation.\n",
      "Epoch: 1860, begin evaluation.\n",
      "Epoch: 1870, begin evaluation.\n",
      "Epoch: 1880, begin evaluation.\n",
      "Epoch: 1890, begin evaluation.\n",
      "Epoch: 1900, begin evaluation.\n",
      "Epoch: 1910, begin evaluation.\n",
      "Epoch: 1920, begin evaluation.\n",
      "Epoch: 1930, begin evaluation.\n",
      "Epoch: 1940, begin evaluation.\n",
      "Epoch: 1950, begin evaluation.\n",
      "Epoch: 1960, begin evaluation.\n",
      "Epoch: 1970, begin evaluation.\n",
      "Epoch: 1980, begin evaluation.\n",
      "Epoch: 1990, begin evaluation.\n",
      "0.0001478397048761231\n",
      "Epoch: 0, begin evaluation.\n",
      "Epoch: 10, begin evaluation.\n",
      "Epoch: 20, begin evaluation.\n",
      "Epoch: 30, begin evaluation.\n",
      "Epoch: 40, begin evaluation.\n",
      "Epoch: 50, begin evaluation.\n",
      "Epoch: 60, begin evaluation.\n",
      "Epoch: 70, begin evaluation.\n",
      "Epoch: 80, begin evaluation.\n",
      "Epoch: 90, begin evaluation.\n",
      "Epoch: 100, begin evaluation.\n",
      "Epoch: 110, begin evaluation.\n",
      "Epoch: 120, begin evaluation.\n",
      "Epoch: 130, begin evaluation.\n",
      "Epoch: 140, begin evaluation.\n",
      "Epoch: 150, begin evaluation.\n",
      "Epoch: 160, begin evaluation.\n",
      "Epoch: 170, begin evaluation.\n",
      "Epoch: 180, begin evaluation.\n",
      "Epoch: 190, begin evaluation.\n",
      "Epoch: 200, begin evaluation.\n",
      "Epoch: 210, begin evaluation.\n",
      "Epoch: 220, begin evaluation.\n",
      "Epoch: 230, begin evaluation.\n",
      "Epoch: 240, begin evaluation.\n",
      "Epoch: 250, begin evaluation.\n",
      "Epoch: 260, begin evaluation.\n",
      "Epoch: 270, begin evaluation.\n",
      "Epoch: 280, begin evaluation.\n",
      "Epoch: 290, begin evaluation.\n",
      "Epoch: 300, begin evaluation.\n",
      "Epoch: 310, begin evaluation.\n",
      "Epoch: 320, begin evaluation.\n",
      "Epoch: 330, begin evaluation.\n",
      "Epoch: 340, begin evaluation.\n",
      "Epoch: 350, begin evaluation.\n",
      "Epoch: 360, begin evaluation.\n",
      "Epoch: 370, begin evaluation.\n",
      "Epoch: 380, begin evaluation.\n",
      "Epoch: 390, begin evaluation.\n",
      "Epoch: 400, begin evaluation.\n",
      "Epoch: 410, begin evaluation.\n",
      "Epoch: 420, begin evaluation.\n",
      "Epoch: 430, begin evaluation.\n",
      "Epoch: 440, begin evaluation.\n",
      "Epoch: 450, begin evaluation.\n",
      "Epoch: 460, begin evaluation.\n",
      "Epoch: 470, begin evaluation.\n",
      "Epoch: 480, begin evaluation.\n",
      "Epoch: 490, begin evaluation.\n",
      "Epoch: 500, begin evaluation.\n",
      "Epoch: 510, begin evaluation.\n",
      "Epoch: 520, begin evaluation.\n",
      "Epoch: 530, begin evaluation.\n",
      "Epoch: 540, begin evaluation.\n",
      "Epoch: 550, begin evaluation.\n",
      "Epoch: 560, begin evaluation.\n",
      "Epoch: 570, begin evaluation.\n",
      "Epoch: 580, begin evaluation.\n",
      "Epoch: 590, begin evaluation.\n",
      "Epoch: 600, begin evaluation.\n",
      "Epoch: 610, begin evaluation.\n",
      "Epoch: 620, begin evaluation.\n",
      "Epoch: 630, begin evaluation.\n",
      "Epoch: 640, begin evaluation.\n",
      "Epoch: 650, begin evaluation.\n",
      "Epoch: 660, begin evaluation.\n",
      "Epoch: 670, begin evaluation.\n",
      "Epoch: 680, begin evaluation.\n",
      "Epoch: 690, begin evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 700, begin evaluation.\n",
      "Epoch: 710, begin evaluation.\n",
      "Epoch: 720, begin evaluation.\n",
      "Epoch: 730, begin evaluation.\n",
      "Epoch: 740, begin evaluation.\n",
      "Epoch: 750, begin evaluation.\n",
      "Epoch: 760, begin evaluation.\n",
      "Epoch: 770, begin evaluation.\n",
      "Epoch: 780, begin evaluation.\n",
      "Epoch: 790, begin evaluation.\n",
      "Epoch: 800, begin evaluation.\n",
      "Epoch: 810, begin evaluation.\n",
      "Epoch: 820, begin evaluation.\n",
      "Epoch: 830, begin evaluation.\n",
      "Epoch: 840, begin evaluation.\n",
      "Epoch: 850, begin evaluation.\n",
      "Epoch: 860, begin evaluation.\n",
      "Epoch: 870, begin evaluation.\n",
      "Epoch: 880, begin evaluation.\n",
      "Epoch: 890, begin evaluation.\n",
      "Epoch: 900, begin evaluation.\n",
      "Epoch: 910, begin evaluation.\n",
      "Epoch: 920, begin evaluation.\n",
      "Epoch: 930, begin evaluation.\n",
      "Epoch: 940, begin evaluation.\n",
      "Epoch: 950, begin evaluation.\n",
      "Epoch: 960, begin evaluation.\n",
      "Epoch: 970, begin evaluation.\n",
      "Epoch: 980, begin evaluation.\n",
      "Epoch: 990, begin evaluation.\n",
      "Epoch: 1000, begin evaluation.\n",
      "Epoch: 1010, begin evaluation.\n",
      "Epoch: 1020, begin evaluation.\n",
      "Epoch: 1030, begin evaluation.\n",
      "Epoch: 1040, begin evaluation.\n",
      "Epoch: 1050, begin evaluation.\n",
      "Epoch: 1060, begin evaluation.\n",
      "Epoch: 1070, begin evaluation.\n",
      "Epoch: 1080, begin evaluation.\n",
      "Epoch: 1090, begin evaluation.\n",
      "Epoch: 1100, begin evaluation.\n",
      "Epoch: 1110, begin evaluation.\n",
      "Epoch: 1120, begin evaluation.\n",
      "Epoch: 1130, begin evaluation.\n",
      "Epoch: 1140, begin evaluation.\n",
      "Epoch: 1150, begin evaluation.\n",
      "Epoch: 1160, begin evaluation.\n",
      "Epoch: 1170, begin evaluation.\n",
      "Epoch: 1180, begin evaluation.\n",
      "Epoch: 1190, begin evaluation.\n",
      "Epoch: 1200, begin evaluation.\n",
      "Epoch: 1210, begin evaluation.\n",
      "Epoch: 1220, begin evaluation.\n",
      "Epoch: 1230, begin evaluation.\n",
      "Epoch: 1240, begin evaluation.\n",
      "Epoch: 1250, begin evaluation.\n",
      "Epoch: 1260, begin evaluation.\n",
      "Epoch: 1270, begin evaluation.\n",
      "Epoch: 1280, begin evaluation.\n",
      "Epoch: 1290, begin evaluation.\n",
      "Epoch: 1300, begin evaluation.\n",
      "Epoch: 1310, begin evaluation.\n",
      "Epoch: 1320, begin evaluation.\n",
      "Epoch: 1330, begin evaluation.\n",
      "Epoch: 1340, begin evaluation.\n",
      "Epoch: 1350, begin evaluation.\n",
      "Epoch: 1360, begin evaluation.\n",
      "Epoch: 1370, begin evaluation.\n",
      "Epoch: 1380, begin evaluation.\n",
      "Epoch: 1390, begin evaluation.\n",
      "Epoch: 1400, begin evaluation.\n",
      "Epoch: 1410, begin evaluation.\n",
      "Epoch: 1420, begin evaluation.\n",
      "Epoch: 1430, begin evaluation.\n",
      "Epoch: 1440, begin evaluation.\n",
      "Epoch: 1450, begin evaluation.\n",
      "Epoch: 1460, begin evaluation.\n",
      "Epoch: 1470, begin evaluation.\n",
      "Epoch: 1480, begin evaluation.\n",
      "Epoch: 1490, begin evaluation.\n",
      "Epoch: 1500, begin evaluation.\n",
      "Epoch: 1510, begin evaluation.\n",
      "Epoch: 1520, begin evaluation.\n",
      "Epoch: 1530, begin evaluation.\n",
      "Epoch: 1540, begin evaluation.\n",
      "Epoch: 1550, begin evaluation.\n",
      "Epoch: 1560, begin evaluation.\n",
      "Epoch: 1570, begin evaluation.\n",
      "Epoch: 1580, begin evaluation.\n",
      "Epoch: 1590, begin evaluation.\n",
      "Epoch: 1600, begin evaluation.\n",
      "Epoch: 1610, begin evaluation.\n",
      "Epoch: 1620, begin evaluation.\n",
      "Epoch: 1630, begin evaluation.\n",
      "Epoch: 1640, begin evaluation.\n",
      "Epoch: 1650, begin evaluation.\n",
      "Epoch: 1660, begin evaluation.\n",
      "Epoch: 1670, begin evaluation.\n",
      "Epoch: 1680, begin evaluation.\n",
      "Epoch: 1690, begin evaluation.\n",
      "Epoch: 1700, begin evaluation.\n",
      "Epoch: 1710, begin evaluation.\n",
      "Epoch: 1720, begin evaluation.\n",
      "Epoch: 1730, begin evaluation.\n",
      "Epoch: 1740, begin evaluation.\n",
      "Epoch: 1750, begin evaluation.\n",
      "Epoch: 1760, begin evaluation.\n",
      "Epoch: 1770, begin evaluation.\n",
      "Epoch: 1780, begin evaluation.\n",
      "Epoch: 1790, begin evaluation.\n",
      "Epoch: 1800, begin evaluation.\n",
      "Epoch: 1810, begin evaluation.\n",
      "Epoch: 1820, begin evaluation.\n",
      "Epoch: 1830, begin evaluation.\n",
      "Epoch: 1840, begin evaluation.\n",
      "Epoch: 1850, begin evaluation.\n",
      "Epoch: 1860, begin evaluation.\n",
      "Epoch: 1870, begin evaluation.\n",
      "Epoch: 1880, begin evaluation.\n",
      "Epoch: 1890, begin evaluation.\n",
      "Epoch: 1900, begin evaluation.\n",
      "Epoch: 1910, begin evaluation.\n",
      "Epoch: 1920, begin evaluation.\n",
      "Epoch: 1930, begin evaluation.\n",
      "Epoch: 1940, begin evaluation.\n",
      "Epoch: 1950, begin evaluation.\n",
      "Epoch: 1960, begin evaluation.\n",
      "Epoch: 1970, begin evaluation.\n",
      "Epoch: 1980, begin evaluation.\n",
      "Epoch: 1990, begin evaluation.\n",
      "2.1790737586892042e-05\n",
      "Epoch: 0, begin evaluation.\n",
      "Epoch: 10, begin evaluation.\n",
      "Epoch: 20, begin evaluation.\n",
      "Epoch: 30, begin evaluation.\n",
      "Epoch: 40, begin evaluation.\n",
      "Epoch: 50, begin evaluation.\n",
      "Epoch: 60, begin evaluation.\n",
      "Epoch: 70, begin evaluation.\n",
      "Epoch: 80, begin evaluation.\n",
      "Epoch: 90, begin evaluation.\n",
      "Epoch: 100, begin evaluation.\n",
      "Epoch: 110, begin evaluation.\n",
      "Epoch: 120, begin evaluation.\n",
      "Epoch: 130, begin evaluation.\n",
      "Epoch: 140, begin evaluation.\n",
      "Epoch: 150, begin evaluation.\n",
      "Epoch: 160, begin evaluation.\n",
      "Epoch: 170, begin evaluation.\n",
      "Epoch: 180, begin evaluation.\n",
      "Epoch: 190, begin evaluation.\n",
      "Epoch: 200, begin evaluation.\n",
      "Epoch: 210, begin evaluation.\n",
      "Epoch: 220, begin evaluation.\n",
      "Epoch: 230, begin evaluation.\n",
      "Epoch: 240, begin evaluation.\n",
      "Epoch: 250, begin evaluation.\n",
      "Epoch: 260, begin evaluation.\n",
      "Epoch: 270, begin evaluation.\n",
      "Epoch: 280, begin evaluation.\n",
      "Epoch: 290, begin evaluation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 252\u001b[0m\n\u001b[0;32m    250\u001b[0m             writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL2 error \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m i, err, epoch)\n\u001b[0;32m    251\u001b[0m         pinn\u001b[38;5;241m.\u001b[39mtrain()    \n\u001b[1;32m--> 252\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m i, \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem(), epoch)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mprint\u001b[39m(err)\n\u001b[0;32m    254\u001b[0m err_all\u001b[38;5;241m.\u001b[39mappend(err)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\lhb\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\lhb\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\lhb\\lib\\site-packages\\torch\\optim\\lbfgs.py:390\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_old):\n\u001b[0;32m    389\u001b[0m         be_i \u001b[38;5;241m=\u001b[39m old_dirs[i]\u001b[38;5;241m.\u001b[39mdot(r) \u001b[38;5;241m*\u001b[39m ro[i]\n\u001b[1;32m--> 390\u001b[0m         \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_stps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mal\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbe_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_flat_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m     prev_flat_grad \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import bisect\n",
    "import numpy as np\n",
    "torch.manual_seed(1)\n",
    "err_all=[]\n",
    "for i in range(6):\n",
    "    print(i)\n",
    "    class PINN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            if i==0:\n",
    "                self.model1 = nn.Sequential(\n",
    "                    nn.Linear(2, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 2)\n",
    "                )\n",
    "            elif i==1:\n",
    "                self.model1 = nn.Sequential(\n",
    "                    nn.Linear(2, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 2)\n",
    "                )\n",
    "            elif i==2:\n",
    "                self.model1 = nn.Sequential(\n",
    "                    nn.Linear(2, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 2)\n",
    "                )\n",
    "            elif i==3:\n",
    "                self.model1 = nn.Sequential(\n",
    "                    nn.Linear(2, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 2)\n",
    "                )\n",
    "            elif i==4:\n",
    "                self.model1 = nn.Sequential(\n",
    "                    nn.Linear(2, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 2)\n",
    "                )\n",
    "            elif i==5:\n",
    "                elf.model1 = nn.Sequential(\n",
    "                    nn.Linear(2, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(100, 2)\n",
    "                )\n",
    "            \n",
    "        def forward(self, x):\n",
    "            output = self.model1(x)\n",
    "            return output\n",
    "\n",
    "\n",
    "    class MSE0b(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, x, y):\n",
    "            output = torch.sum((x - y) ** 2)/len(x)\n",
    "            return output\n",
    "\n",
    "\n",
    "    class MSEf(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, x):\n",
    "            output = torch.sum(x ** 2)/len(x)\n",
    "            return output\n",
    "\n",
    "\n",
    "    # 模型初始化\n",
    "    pinn = PINN().cuda()\n",
    "    optimizer = torch.optim.LBFGS(pinn.parameters(), lr=0.2)\n",
    "    init_loss = MSE0b().cuda()\n",
    "    boundary_loss = MSE0b().cuda()\n",
    "    internal_loss = MSEf().cuda()\n",
    "    \n",
    "        # 初值训练点\n",
    "    N0 = 50\n",
    "    init_data = (torch.rand(N0, 2) - 0.5) * 10\n",
    "    init_data[:, 0:1] = torch.zeros((N0, 1))\n",
    "    init_data = init_data.cuda()\n",
    "    # init_data.requires_grad_()\n",
    "\n",
    "    init_target = torch.zeros((N0, 2))\n",
    "    init_target[:, 0:1] = 2 / torch.cosh(init_data[:, 1:2])\n",
    "    init_target = init_target.cuda()\n",
    "\n",
    "    # 边界训练点\n",
    "    Nb = 50\n",
    "    sample_time = torch.rand((Nb, 1))*torch.pi/2\n",
    "    bd1_data = torch.full((Nb, 2), 5.)\n",
    "    bd1_data[:, 0:1] = sample_time\n",
    "    bd2_data = torch.full((Nb, 2), -5.)\n",
    "    bd2_data[:, 0:1] = sample_time\n",
    "    bd1_data = bd1_data.cuda()\n",
    "    bd2_data = bd2_data.cuda()\n",
    "    # bd1_data.requires_grad_()\n",
    "    # bd2_data.requires_grad_()\n",
    "\n",
    "    # 内部训练点\n",
    "    Nf = 20000\n",
    "    sample_x = (torch.rand(Nf, 1) - 0.5) * 10\n",
    "    sample_time2 = torch.rand((Nf, 1))*torch.pi/2\n",
    "    internal_data = torch.zeros((Nf, 2))\n",
    "    internal_data[:, 0:1] = sample_time2\n",
    "    internal_data[:, 1:2] = sample_x\n",
    "    internal_data = internal_data.cuda()\n",
    "    # internal_data.requires_grad_()\n",
    "\n",
    "    # 求导梯度\n",
    "    bd_u_grad_out = torch.Tensor([[1, 0]*Nb]).reshape((-1, 2)).cuda()\n",
    "    bd_v_grad_out = torch.Tensor([[0, 1]*Nb]).reshape((-1, 2)).cuda()\n",
    "\n",
    "    internal_u_grad_out = torch.Tensor([[1, 0]*Nf]).reshape((-1, 2)).cuda()\n",
    "    internal_v_grad_out = torch.Tensor([[0, 1]*Nf]).reshape((-1, 2)).cuda()\n",
    "    \n",
    "    pinn.train()\n",
    "    def closure():\n",
    "        # 初值误差\n",
    "        init_data_ = init_data.clone()\n",
    "        init_data_.requires_grad_()\n",
    "        init_y = pinn(init_data_)\n",
    "        mse0 = init_loss(init_y, init_target)\n",
    "\n",
    "        # 边值误差\n",
    "        bd1_data_ = bd1_data.clone()\n",
    "        bd1_data_.requires_grad_()\n",
    "        bd2_data_ = bd2_data.clone()\n",
    "        bd2_data_.requires_grad_()\n",
    "        bd1_y = pinn(bd1_data_)\n",
    "        bd2_y = pinn(bd2_data_)\n",
    "\n",
    "        bd1_dudx = torch.autograd.grad(outputs=bd1_y, inputs=bd1_data_, grad_outputs=bd_u_grad_out, create_graph=True)[0][:, 1:2]\n",
    "        bd1_dvdx = torch.autograd.grad(outputs=bd1_y, inputs=bd1_data_, grad_outputs=bd_v_grad_out, create_graph=True)[0][:, 1:2]\n",
    "\n",
    "        bd2_dudx = torch.autograd.grad(outputs=bd2_y, inputs=bd2_data_, grad_outputs=bd_u_grad_out, create_graph=True)[0][:, 1:2]\n",
    "        bd2_dvdx = torch.autograd.grad(outputs=bd2_y, inputs=bd2_data_, grad_outputs=bd_v_grad_out, create_graph=True)[0][:, 1:2]\n",
    "\n",
    "        mseb = boundary_loss(bd1_dudx, bd2_dudx) + boundary_loss(bd1_dvdx, bd2_dvdx)\n",
    "\n",
    "        # 内点误差\n",
    "        internal_data_ = internal_data.clone()\n",
    "        internal_data_.requires_grad_()\n",
    "        internal_y = pinn(internal_data_)\n",
    "\n",
    "        internal_du = torch.autograd.grad(outputs=internal_y, inputs=internal_data_, grad_outputs=internal_u_grad_out, create_graph=True)[0]\n",
    "        internal_dv = torch.autograd.grad(outputs=internal_y, inputs=internal_data_, grad_outputs=internal_v_grad_out, create_graph=True)[0]\n",
    "\n",
    "        internal_dudt = internal_du[:, 0:1]\n",
    "        internal_dudx = internal_du[:, 1:2]\n",
    "        internal_dvdt = internal_dv[:, 0:1]\n",
    "        internal_dvdx = internal_dv[:, 1:2]\n",
    "\n",
    "        internal_ddudxx = torch.autograd.grad(outputs=internal_dudx, inputs=internal_data_, grad_outputs=torch.ones_like(internal_dudx), create_graph=True)[0][:, 1:2]\n",
    "        internal_ddvdxx = torch.autograd.grad(outputs=internal_dvdx, inputs=internal_data_, grad_outputs=torch.ones_like(internal_dudx), create_graph=True)[0][:, 1:2]\n",
    "\n",
    "        uv_square = internal_y[:, 0:1] ** 2 + internal_y[:, 1:2] ** 2\n",
    "        f_real = 0.5 * internal_ddudxx - internal_dvdt + uv_square * internal_y[:, 0:1]\n",
    "        f_imag = 0.5 * internal_ddvdxx + internal_dudt + uv_square * internal_y[:, 1:2]\n",
    "\n",
    "        msef = internal_loss(f_real) + internal_loss(f_imag)\n",
    "\n",
    "        # 总共误差\n",
    "        total_mse = mse0 + mseb + msef\n",
    "\n",
    "        # 清空梯度+反向传播\n",
    "        optimizer.zero_grad()\n",
    "        total_mse.backward()\n",
    "\n",
    "        return total_mse\n",
    "\n",
    "    writer = SummaryWriter(\"PINN_log_xlayers_100neuron\")\n",
    "    #for epoch in range(2000):\n",
    "    #    if epoch % 10 == 0 :\n",
    "    #        print(\"Epoch: {}\".format(epoch))\n",
    "    #    writer.add_scalar(\"train_%d\" % N, optimizer.step(closure).item(), epoch)\n",
    "\n",
    "    #writer.close()\n",
    "    \n",
    "    exact_data = loadmat(\"NLS.mat\")\n",
    "    idx = bisect.bisect_right(exact_data['tt'][0], 1.00)\n",
    "    t = exact_data['tt'][0][idx]\n",
    "    x = exact_data['uu'][:, idx]\n",
    "    x_axis = exact_data['x'][0]\n",
    "\n",
    "    pinn.train(False)\n",
    "\n",
    "    for epoch in range(2000):\n",
    "        if epoch % 10 == 0 :\n",
    "            pinn.train(False)\n",
    "            print(\"Epoch: {}, begin evaluation.\".format(epoch))\n",
    "            with torch.no_grad():\n",
    "                test_data = torch.full((len(x), 2), t)\n",
    "                test_data[:, 1:2] = torch.tensor(exact_data['x'].T)\n",
    "                test_data = test_data.cuda()\n",
    "                result = pinn(test_data).cpu()\n",
    "                real_part = np.asarray(result[:, 0])\n",
    "                imag_part = np.asarray(result[:, 1])\n",
    "                ext_real, ext_imag = x.real, x.imag\n",
    "                err = ((np.abs(real_part - ext_real)**2).sum()+ (np.abs(imag_part - ext_imag)**2).sum()) / len(x_axis)\n",
    "                writer.add_scalar(\"L2 error %d\" % i, err, epoch)\n",
    "            pinn.train()    \n",
    "        writer.add_scalar(\"train %d\" % i, optimizer.step(closure).item(), epoch)\n",
    "    print(err)\n",
    "    err_all.append(err)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3851cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
